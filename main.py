#!/usr/bin/env python3
"""
Google Maps Places Crawler - Main Entry Point for Render Deployment
Tá»± Ä‘á»™ng táº¡o database tables vÃ  crawl dá»¯ liá»‡u tá»« Google Maps
"""

import os
import sys
import asyncio
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
import importlib.util

# Import checkpoint system
from checkpoint_system import checkpoint

# Load environment variables
load_dotenv()

def get_db_config():
    """Láº¥y cáº¥u hÃ¬nh database tá»« environment variables"""
    # Æ¯u tiÃªn DATABASE_URL tá»« Render
    database_url = os.getenv('DATABASE_URL')
    if database_url:
        return {'connection_string': database_url}
    
    # Fallback cho local development
    return {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ggmaps'),
        'user': os.getenv('DB_USER', 'ggmaps'),
        'password': os.getenv('DB_PASSWORD', 'ggmaps')
    }

def create_tables():
    """Táº¡o cÃ¡c báº£ng trong database náº¿u chÆ°a tá»“n táº¡i"""
    print("ğŸ”§ Creating database tables...")
    
    conn = None
    try:
        db_config = get_db_config()
        
        # Sá»­ dá»¥ng connection string hoáº·c individual parameters
        if 'connection_string' in db_config:
            conn = psycopg2.connect(db_config['connection_string'])
        else:
            conn = psycopg2.connect(**db_config)
            
        cursor = conn.cursor()
        
        # Kiá»ƒm tra tables Ä‘Ã£ tá»“n táº¡i chÆ°a
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        existing_tables = [table[0] for table in cursor.fetchall()]
        
        if 'place' in existing_tables and 'review' in existing_tables:
            print("âœ… Database tables already exist!")
            print(f"ğŸ“Š Existing tables: {existing_tables}")
            return
        
        # Äá»c file SQL vÃ  táº¡o tables
        with open('create_tables.sql', 'r', encoding='utf-8') as f:
            sql_content = f.read()
        
        # Thá»±c thi SQL Ä‘á»ƒ táº¡o tables
        cursor.execute(sql_content)
        conn.commit()
        
        print("âœ… Database tables created successfully!")
        
        # Kiá»ƒm tra tables Ä‘Ã£ táº¡o
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        tables = cursor.fetchall()
        print(f"ğŸ“Š Tables created: {[table[0] for table in tables]}")
        
    except Exception as e:
        print(f"âŒ Error creating tables: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            cursor.close()
            conn.close()

def check_database_connection():
    """Kiá»ƒm tra káº¿t ná»‘i database"""
    print("ğŸ” Checking database connection...")
    
    try:
        db_config = get_db_config()
        
        # Sá»­ dá»¥ng connection string hoáº·c individual parameters
        if 'connection_string' in db_config:
            conn = psycopg2.connect(db_config['connection_string'])
        else:
            conn = psycopg2.connect(**db_config)
            
        cursor = conn.cursor()
        cursor.execute("SELECT version();")
        version = cursor.fetchone()[0]
        print(f"âœ… Database connected successfully!")
        print(f"ğŸ“Š PostgreSQL version: {version}")
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        print(f"âŒ Database connection error: {e}")
        return False

def load_crawler_module():
    """Load crawler module tá»« file cÃ³ tÃªn Ä‘áº·c biá»‡t"""
    try:
        spec = importlib.util.spec_from_file_location("crawl_info_place", "crawl_info_place (1).py")
        crawl_module = importlib.util.module_from_spec(spec)
        sys.modules["crawl_info_place"] = crawl_module
        spec.loader.exec_module(crawl_module)
        return crawl_module
    except Exception as e:
        print(f"âŒ Error loading crawler module: {e}")
        return None

async def run_crawler():
    """Cháº¡y crawler vá»›i checkpoint system Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u"""
    print("ğŸš€ Starting Google Maps Places Crawler...")
    print("ğŸ“ Target: Quáº­n 1 & Quáº­n 2")
    print("=" * 60)
    
    # Load crawler module
    crawl_module = load_crawler_module()
    if not crawl_module:
        print("âŒ Failed to load crawler module")
        return
    
    # Load URLs tá»« CSV files
    all_urls = crawl_module.load_urls_from_specific_files()
    
    if not all_urls:
        print("âŒ No URLs found in CSV files!")
        return
    
    # Kiá»ƒm tra checkpoint vÃ  láº¥y URLs cÃ²n láº¡i
    remaining_urls = checkpoint.get_remaining_urls(all_urls)
    
    if not remaining_urls:
        print("âœ… All URLs have been processed!")
        progress = checkpoint.get_progress_summary()
        print(f"ğŸ“Š Progress: {progress['processed']}/{progress['total']} URLs completed")
        return
    
    # Khá»Ÿi táº¡o checkpoint náº¿u chÆ°a cÃ³
    if checkpoint.data["status"] == "not_started":
        checkpoint.start_crawl(len(all_urls))
    
    print(f"ğŸ“Š Total URLs: {len(all_urls)}")
    print(f"ğŸ“Š Remaining URLs: {len(remaining_urls)}")
    print(f"ğŸ“Š Progress: {checkpoint.get_progress_summary()['progress_percent']}%")
    print("=" * 60)
    
    # Import playwright functions
    from playwright.async_api import async_playwright
    
    try:
        async with async_playwright() as playwright:
            data = await crawl_module.open_place_pages_with_checkpoint(playwright, remaining_urls)
        
        # ÄÃ¡nh dáº¥u hoÃ n thÃ nh
        checkpoint.complete_crawl()
        
    except Exception as e:
        print(f"âŒ Crawler error: {e}")
        print("ğŸ’¾ Checkpoint saved - can resume from last processed URL")
        raise
    
    # Thá»‘ng kÃª káº¿t quáº£
    print("\n" + "=" * 60)
    print("ğŸ‰ Crawling completed!")
    print("=" * 60)
    progress = checkpoint.get_progress_summary()
    print(f"ğŸ“Š Total places processed: {progress['processed']}")
    print(f"ğŸ“Š Failed URLs: {progress['failed']}")
    print(f"ğŸ“Š Progress: {progress['progress_percent']}%")
    
    if progress['processed'] > 0:
        print(f"ğŸ’¾ All data has been saved to PostgreSQL database")
        db_config = get_db_config()
        print(f"ğŸ”— Database: {db_config['host']}:{db_config['port']}/{db_config['database']}")

def main():
    """Main function - entry point cho Render"""
    print("ğŸŒŸ Google Maps Places Crawler - Render Deployment")
    print("=" * 60)
    
    # Kiá»ƒm tra environment variables
    print("ğŸ” Checking environment variables...")
    db_config = get_db_config()
    print(f"ğŸ“Š Database config: {db_config['host']}:{db_config['port']}/{db_config['database']}")
    
    # Kiá»ƒm tra káº¿t ná»‘i database
    if not check_database_connection():
        print("âŒ Cannot connect to database. Please check your environment variables.")
        sys.exit(1)
    
    # Táº¡o tables
    try:
        create_tables()
    except Exception as e:
        print(f"âŒ Failed to create tables: {e}")
        sys.exit(1)
    
    # Cháº¡y crawler
    try:
        asyncio.run(run_crawler())
    except Exception as e:
        print(f"âŒ Crawler failed: {e}")
        sys.exit(1)
    
    print("\nğŸ‰ All tasks completed successfully!")
    print("=" * 60)

if __name__ == "__main__":
    main()
